
---
title: "Airflow Data Pipeline Tutorial"
author: "Jamie Yan"
output: pdf_document
---

# Introduction

This tutorial explains how to set up an Apache Airflow data pipeline using Docker, define a DAG to process and load data, and write a Python script to handle the database operations. The tutorial includes detailed explanations and code snippets.

---

# Section 1: Docker Compose Setup

To set up your environment, create a `docker-compose.yml` file with the following configuration. Replace placeholders like `<YOUR_POSTGRES_USER>` and `<YOUR_POSTGRES_PASSWORD>` with your credentials.

```yaml
version: '3.9'
services:
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=<YOUR_POSTGRES_USER>
      - POSTGRES_PASSWORD=<YOUR_POSTGRES_PASSWORD>
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U <YOUR_POSTGRES_USER>"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-init:
    image: apache/airflow:2.10.3
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://<YOUR_POSTGRES_USER>:<YOUR_POSTGRES_PASSWORD>@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=<YOUR_FERNET_KEY>
    entrypoint: "bash -c 'airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname Admin --role Admin --email admin@example.com'"
    depends_on:
      postgres:
        condition: service_healthy

  airflow-webserver:
    image: apache/airflow:2.10.3
    environment:
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://<YOUR_POSTGRES_USER>:<YOUR_POSTGRES_PASSWORD>@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=<YOUR_FERNET_KEY>
    ports:
      - "8080:8080"
    command: "airflow webserver"
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./scripts:/opt/airflow/scripts
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-scheduler:
    image: apache/airflow:2.10.3
    environment:
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://<YOUR_POSTGRES_USER>:<YOUR_POSTGRES_PASSWORD>@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=<YOUR_FERNET_KEY>
    command: "airflow scheduler"
    depends_on:
      airflow-webserver:
        condition: service_healthy
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./scripts:/opt/airflow/scripts
```

---

# Section 2: Define the Airflow DAG

The DAG (`data_pipeline.py`) defines the workflow for downloading, processing, and inserting data. Replace placeholders with your paths and endpoint details.

```python
from airflow import DAG
from airflow.providers.http.operators.http import HttpOperator
from airflow.operators.python import PythonOperator
import base64
from datetime import datetime, timedelta
import io
import zipfile
import pandas as pd
from airflow.models import Connection
from airflow import settings
import subprocess
import os

OUTPUT_DIR = "/opt/airflow/dags"
DELIMITER = '|'

default_args = {
    "owner": "airflow",
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

def process_zip_data(ti, output_dir, delimiter):
    encoded_content = ti.xcom_pull(task_ids="download_zip_file")
    zip_content = base64.b64decode(encoded_content)
    zip_content_io = io.BytesIO(zip_content)
    with zipfile.ZipFile(zip_content_io, 'r') as zip_file:
        for file_name in zip_file.namelist():
            if file_name.endswith('.csv'):
                with zip_file.open(file_name) as csv_file:
                    df = pd.read_csv(csv_file, sep=',')
                    output_file_path = f"{output_dir}/processed_{file_name}"
                    df.to_csv(output_file_path, index=False, sep=delimiter)

def run_csv_to_db_script():
    try:
        result = subprocess.run(
            ["python3", "/opt/airflow/scripts/csv_to_db_stage.py"],
            check=True,
            capture_output=True,
            text=True,
            env=os.environ.copy(),
        )
        print("CSV to DB script output:", result.stdout)
    except subprocess.CalledProcessError as e:
        print("CSV to DB script failed:", e.stderr)
        raise RuntimeError(f"CSV to DB script failed with error: {e.stderr}")

with DAG(
    "data_pipeline_dag",
    default_args=default_args,
    description="A data pipeline that downloads a zip file, extracts CSV, and saves to files",
    schedule_interval=None,
    start_date=datetime(2024, 11, 11),
    catchup=False,
) as dag:
    download_zip_file = HttpOperator(
        task_id="download_zip_file",
        method="GET",
        http_conn_id="cms_data",
        endpoint="/path/to/your/zipfile.zip",
        response_filter=lambda response: base64.b64encode(response.content).decode("utf-8"),
        log_response=True,
        do_xcom_push=True,
    )

    process_and_save_csv = PythonOperator(
        task_id="process_and_save_csv",
        python_callable=process_zip_data,
        op_kwargs={
            "output_dir": OUTPUT_DIR,
            "delimiter": DELIMITER
        },
    )

    insert_into_db = PythonOperator(
        task_id="insert_into_db",
        python_callable=run_csv_to_db_script,
    )

    download_zip_file >> process_and_save_csv >> insert_into_db
```

---

# Section 3: Database Script

This script loads processed CSV data into a PostgreSQL database. Replace database connection details with your own.

```python
import pandas as pd
import psycopg2
from psycopg2 import sql
import os
from sqlalchemy.engine.url import make_url

db_url = make_url(os.environ.get('AIRFLOW__EXTRA__DB_CONN'))
DB_HOST = db_url.host
DB_PORT = db_url.port
DB_NAME = db_url.database
DB_USER = db_url.username
DB_PASSWORD = db_url.password
CSV_FILE = "/opt/airflow/dags/processed_outpatient.csv"
TABLE_NAME = "staging_table"

df = pd.read_csv(CSV_FILE, low_memory=False)

def generate_create_table_query(df, table_name):
    columns = []
    for col, dtype in zip(df.columns, df.dtypes):
        pg_type = "TEXT"
        if "int" in str(dtype):
            pg_type = "NUMERIC"
        elif "float" in str(dtype):
            pg_type = "FLOAT"
        elif "datetime" in str(dtype):
            pg_type = "TIMESTAMP"
        columns.append(f"{col} {pg_type}")
    return f"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(columns)});"

create_table_query = generate_create_table_query(df, TABLE_NAME)

try:
    conn = psycopg2.connect(
        host=DB_HOST,
        port=DB_PORT,
        dbname=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD,
    )
    cursor = conn.cursor()
    cursor.execute(create_table_query)
    conn.commit()

    columns = ",".join(df.columns)
    placeholders = ",".join(["%s"] * len(df.columns))
    insert_query = sql.SQL(f"INSERT INTO {TABLE_NAME} ({columns}) VALUES ({placeholders})")
    batch_size = 10000
    for i in range(0, len(df), batch_size):
        batch = df.iloc[i : i + batch_size]
        cursor.executemany(insert_query, batch.values.tolist())
        conn.commit()
except Exception as e:
    print("Error:", e)
finally:
    if cursor:
        cursor.close()
    if conn:
        conn.close()
```
